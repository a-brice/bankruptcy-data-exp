{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c015df1",
   "metadata": {},
   "source": [
    "# Model analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8fd8ba",
   "metadata": {},
   "source": [
    "#### Required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0af2c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3df31d",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00d9f253",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((43184, 64), Index(['X21', 'X37'], dtype='object'))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../data/extension/csv/data.csv')\n",
    "data.replace('?', np.NAN, inplace=True)\n",
    "data[data.columns.drop(['year', 'X65'])] = data[data.columns.drop(['year', 'X65'])].astype(float)\n",
    "data[['year', 'X65']] = data[['year', 'X65']].astype('category')\n",
    "    \n",
    "\n",
    "useless_col = data.isna().sum()[data.isna().sum() > 5000].index\n",
    "data.drop(useless_col, axis=1, inplace=True)\n",
    "useless_row = data.isna().sum(axis=1)[data.isna().sum(axis=1) > 10].index\n",
    "data.drop(useless_row, axis=0, inplace=True)\n",
    "data.fillna(data.mean(), inplace=True)\n",
    "data = data.sample(frac=1)\n",
    "data.shape, useless_col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82e35b0",
   "metadata": {},
   "source": [
    "#### Train data and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2741e187",
   "metadata": {},
   "source": [
    "To get a balanced dataset that is fairly dependant on each year :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6009c52c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((34545, 64), (8639, 64))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ = pd.DataFrame()\n",
    "test_ = pd.DataFrame()\n",
    "for year in data.year.unique():\n",
    "    tr, ts = train_test_split(data[data.year == year], test_size=0.2, random_state=9)\n",
    "    train_ = train_.append(tr, ignore_index=True)\n",
    "    test_ = test_.append(ts, ignore_index=True)\n",
    "\n",
    "train_.shape, test_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003dd9d5",
   "metadata": {},
   "source": [
    "#### Data standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67ba2f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "categ = ['year', 'X65']\n",
    "\n",
    "scaler =  StandardScaler()\n",
    "scaler.fit(train_.drop(columns=categ))\n",
    "\n",
    "train = pd.DataFrame(scaler.transform(train_.drop(columns=categ)), \n",
    "                      columns=train_.columns.drop(categ))\n",
    "test = pd.DataFrame(scaler.transform(test_.drop(columns=categ)), \n",
    "                     columns=test_.columns.drop(categ))\n",
    "\n",
    "train = pd.concat([train, train_[categ]], axis=1)\n",
    "test = pd.concat([test, test_[categ]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b99febc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = train.pop('X65')\n",
    "train_x = train.copy()\n",
    "\n",
    "test_y = test.pop('X65')\n",
    "test_x = test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7930879",
   "metadata": {},
   "source": [
    "#### Save scaler used for standardize train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d208d59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('saved_model/scaler.sav', 'wb') as f:\n",
    "    pickle.dump(scaler, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6ada41",
   "metadata": {},
   "source": [
    "#### Encode categorical variable : year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e42de2bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((34545, 67), (8639, 67))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x = pd.get_dummies(train_x)\n",
    "test_x = pd.get_dummies(test_x)\n",
    "\n",
    "train_x.shape, test_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0282b33f",
   "metadata": {},
   "source": [
    "### Model testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a19c51d",
   "metadata": {},
   "source": [
    "12 max features because in visualisation part, we saw that only 12 was needed to reach the biggest parts of information contained in dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28ce706",
   "metadata": {},
   "source": [
    "#### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f760457",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.951802  , 0.95035461, 0.95064409, 0.36387321, 0.95035461])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "cross_val_score(lr, train_x, train_y, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed52e1e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(n_jobs=-1)\n",
      "for {'C': 1, 'solver': 'newton-cg'} => mean : 0.9512230424084528\n",
      "for {'C': 1, 'solver': 'sag'} => mean : 0.9512230424084528\n",
      "for {'C': 2, 'solver': 'newton-cg'} => mean : 0.9510204081632653\n",
      "for {'C': 2, 'solver': 'sag'} => mean : 0.9512230424084528\n",
      "for {'C': 3, 'solver': 'newton-cg'} => mean : 0.9510204081632653\n",
      "for {'C': 3, 'solver': 'sag'} => mean : 0.9512230424084528\n",
      "LogisticRegression(C=1, n_jobs=-1, solver='newton-cg')\n"
     ]
    }
   ],
   "source": [
    "params = dict(solver=['newton-cg', 'sag'], C=[1, 2, 3])\n",
    "lr = LogisticRegression(n_jobs=-1)\n",
    "grid = GridSearchCV(lr, params, cv=3, n_jobs=-1)\n",
    "grid.fit(train_x, train_y)\n",
    "grid.cv_results_\n",
    "print(lr)\n",
    "print(\"\\n\".join([f\"for {x} => mean : {y}\" for x,y in zip(\n",
    "                     grid.cv_results_['params'], grid.cv_results_['mean_test_score']\n",
    "                 )]))\n",
    "print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85f1d1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('saved_model/lr.sav', 'wb') as f:\n",
    "    pickle.dump(grid.best_estimator_, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45414d5e",
   "metadata": {},
   "source": [
    "#### Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6caa985c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9510783 , 0.94847301, 0.94963092, 0.35837314, 0.94905196])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = LDA(solver='svd', tol=1e-4)\n",
    "cross_val_score(lda, train_x, train_y, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dea5745a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearDiscriminantAnalysis()\n",
      "for {'solver': 'svd', 'tol': 0.001} => mean : 0.8313214647561151\n",
      "for {'solver': 'svd', 'tol': 0.0001} => mean : 0.8313214647561151\n",
      "for {'solver': 'svd', 'tol': 1e-05} => mean : 0.8313214647561151\n",
      "for {'solver': 'lsqr', 'tol': 0.001} => mean : 0.7702706614560719\n",
      "for {'solver': 'lsqr', 'tol': 0.0001} => mean : 0.7702706614560719\n",
      "for {'solver': 'lsqr', 'tol': 1e-05} => mean : 0.7702706614560719\n",
      "LinearDiscriminantAnalysis(tol=0.001)\n"
     ]
    }
   ],
   "source": [
    "params = dict(solver=['svd', 'lsqr'], tol=[1e-3, 1e-4, 1e-5])\n",
    "lda = LDA()\n",
    "grid = GridSearchCV(lda, params, cv=5, n_jobs=-1)\n",
    "grid.fit(train_x, train_y)\n",
    "grid.cv_results_\n",
    "print(lda)\n",
    "print(\"\\n\".join([f\"for {x} => mean : {y}\" \n",
    "                 for x,y in zip(\n",
    "                     grid.cv_results_['params'], \n",
    "                     grid.cv_results_['mean_test_score'])]\n",
    "               ))\n",
    "print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7641c07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('saved_model/lda.sav', 'wb') as f:\n",
    "    pickle.dump(grid.best_estimator_, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc00385",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5979ff0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96691272, 0.96448111, 0.96552323])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_jobs=-1, random_state=9)\n",
    "cross_val_score(rf, train_x, train_y, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f64c34b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(n_jobs=-1, random_state=9)\n",
      "for {'criterion': 'gini', 'max_depth': 15} => mean : 0.9655811260674483\n",
      "for {'criterion': 'gini', 'max_depth': 45} => mean : 0.9656390215660733\n",
      "for {'criterion': 'gini', 'max_depth': 50} => mean : 0.9656390215660733\n",
      "for {'criterion': 'gini', 'max_depth': 100} => mean : 0.9656390215660733\n",
      "for {'criterion': 'gini', 'max_depth': 200} => mean : 0.9656390215660733\n",
      "RandomForestClassifier(max_depth=45, n_jobs=-1, random_state=9)\n"
     ]
    }
   ],
   "source": [
    "params = dict(criterion=['gini'], max_depth=[15, 45, 50, 100, 200])\n",
    "rf = RandomForestClassifier(n_jobs=-1, random_state=9)\n",
    "\n",
    "grid = GridSearchCV(rf, params, cv=3, n_jobs=-1)\n",
    "grid.fit(train_x, train_y)\n",
    "grid.cv_results_\n",
    "print(rf)\n",
    "print(\"\\n\".join([f\"for {x} => mean : {y}\" \n",
    "                 for x,y in zip(\n",
    "                     grid.cv_results_['params'], \n",
    "                     grid.cv_results_['mean_test_score'])]\n",
    "               ))\n",
    "print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "433da8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('saved_model/rf.sav', 'wb') as f:\n",
    "    pickle.dump(grid.best_estimator_, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225f6f89",
   "metadata": {},
   "source": [
    "#### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "365b6731",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.97119699, 0.96671009, 0.96801274, 0.37067593, 0.96801274])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm = GradientBoostingClassifier()\n",
    "cross_val_score(gbm, train_x, train_y, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9bb4a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier(random_state=9)\n",
      "for {'max_depth': 3, 'n_estimators': 100} => mean : 0.9688522217397596\n",
      "for {'max_depth': 3, 'n_estimators': 300} => mean : 0.9711969894340715\n",
      "for {'max_depth': 5, 'n_estimators': 100} => mean : 0.9718048921696338\n",
      "for {'max_depth': 5, 'n_estimators': 300} => mean : 0.9731364886380084\n",
      "for {'max_depth': 7, 'n_estimators': 100} => mean : 0.9715154146765089\n",
      "for {'max_depth': 7, 'n_estimators': 300} => mean : 0.9737443913735708\n",
      "GradientBoostingClassifier(max_depth=7, n_estimators=300, random_state=9)\n"
     ]
    }
   ],
   "source": [
    "params = dict(n_estimators=[100, 300], max_depth=[3,5,7])\n",
    "gbm = GradientBoostingClassifier(random_state=9)\n",
    "\n",
    "grid = GridSearchCV(gbm, params, cv=3, n_jobs=-1)\n",
    "grid.fit(train_x, train_y)\n",
    "grid.cv_results_\n",
    "print(gbm)\n",
    "print(\"\\n\".join([f\"for {x} => mean : {y}\" \n",
    "                 for x,y in zip(\n",
    "                     grid.cv_results_['params'], \n",
    "                     grid.cv_results_['mean_test_score'])]\n",
    "               ))\n",
    "print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bad173af",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('saved_model/gbm.sav', 'wb') as f:\n",
    "    pickle.dump(grid.best_estimator_, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f99c28",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "506442da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.95238095, 0.95238095, 0.95223621, 0.95238095, 0.95238095])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm = SVC()\n",
    "cross_val_score(svm, train_x, train_y, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159ed603",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict(C=[1, 2], \n",
    "              kernel=['linear', 'rbf'])\n",
    "svm = SVC(random_state=9)\n",
    "\n",
    "grid = GridSearchCV(svm, params, cv=3, n_jobs=-1)\n",
    "grid.fit(train_x, train_y)\n",
    "grid.cv_results_\n",
    "print(svm)\n",
    "print(\"\\n\".join([f\"for {x} => mean : {y}\" \n",
    "                 for x,y in zip(\n",
    "                     grid.cv_results_['params'], \n",
    "                     grid.cv_results_['mean_test_score'])]\n",
    "               ))\n",
    "print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d10a8704",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC()\n",
    "svm.fit(train_x, train_y)\n",
    "with open('saved_model/svm.sav', 'wb') as f:\n",
    "    pickle.dump(svm, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a1694a",
   "metadata": {},
   "source": [
    "#### Bonus neural network : MultiLayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1423e10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9617021276595744"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = MLPClassifier()\n",
    "mlp.fit(train_x, train_y)\n",
    "mlp.score(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c46ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict(hidden_layer_sizes=[100, 120], \n",
    "              activation=['relu', 'tanh'])\n",
    "mlp = MLPClassifier(random_state=9)\n",
    "\n",
    "grid = GridSearchCV(mlp, params, cv=3, n_jobs=-1)\n",
    "grid.fit(train_x, train_y)\n",
    "grid.cv_results_\n",
    "print(mlp)\n",
    "print(\"\\n\".join([f\"for {x} => mean : {y}\" \n",
    "                 for x,y in zip(\n",
    "                     grid.cv_results_['params'], \n",
    "                     grid.cv_results_['mean_test_score'])]\n",
    "               ))\n",
    "print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bcdfd95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('saved_model/mlp.sav', 'wb') as f:\n",
    "    pickle.dump(mlp, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4705506e",
   "metadata": {},
   "source": [
    "### Underlying variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "feeb08e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/udata_description.json', 'r') as f:\n",
    "    udesc = json.loads(f.read())\n",
    "    udesc['U35'] = 'bankrupt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03db885d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((43246, 33), ['financial expenses', 'receivables', 'sales (n) / sales (n-1)'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "udata = pd.read_csv('../data/extension/csv/udata.csv')\n",
    "udata.head()\n",
    "udata.replace(np.inf, np.NAN, inplace=True)\n",
    "useless_col = udata.isna().sum()[udata.isna().sum() > 5000].index\n",
    "udata.drop(useless_col, axis=1, inplace=True)\n",
    "useless_row = udata.isna().sum(axis=1)[udata.isna().sum(axis=1) > 5].index\n",
    "udata.drop(useless_row, axis=0, inplace=True)\n",
    "udata.fillna(udata.mean(), inplace=True)\n",
    "udata.shape, [udesc[var] for var in useless_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "426fe542",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ = pd.DataFrame()\n",
    "test_ = pd.DataFrame()\n",
    "for year in udata.year.unique():\n",
    "    tr, ts = train_test_split(udata[udata.year == year], test_size=0.2, random_state=9)\n",
    "    train_ = train_.append(tr, ignore_index=True)\n",
    "    test_ = test_.append(ts, ignore_index=True)\n",
    "\n",
    "categ = ['year', 'U35']\n",
    "\n",
    "uscaler =  StandardScaler()\n",
    "uscaler.fit(train_.drop(columns=categ))\n",
    "\n",
    "train = pd.DataFrame(uscaler.transform(train_.drop(columns=categ)), \n",
    "                      columns=train_.columns.drop(categ))\n",
    "test = pd.DataFrame(uscaler.transform(test_.drop(columns=categ)), \n",
    "                     columns=test_.columns.drop(categ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567e9d0b",
   "metadata": {},
   "source": [
    "#### Save scaler used for standardize train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c66432a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('saved_model/uscaler.sav', 'wb') as f:\n",
    "    pickle.dump(uscaler, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "076a3940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((34594, 36), (34594,))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.concat([train, train_[categ]], axis=1)\n",
    "test = pd.concat([test, test_[categ]], axis=1)\n",
    "train_y = train.pop('U35')\n",
    "train_x = train.copy()\n",
    "test_y = test.pop('U35')\n",
    "test_x = test.copy()\n",
    "\n",
    "train_x = pd.get_dummies(train_x)\n",
    "test_x = pd.get_dummies(test_x)\n",
    "\n",
    "train_x.shape, train_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98349617",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b3e3c2",
   "metadata": {},
   "source": [
    "#### On logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "286e0862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.95129354, 0.94999277, 0.95100448, 0.95129354, 0.9524429 ])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "cross_val_score(lr, train_x, train_y, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "97741dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(n_jobs=-1)\n",
      "for {'C': 1, 'solver': 'newton-cg'} => mean : 0.7590855965147822\n",
      "for {'C': 1, 'solver': 'sag'} => mean : 0.9512054120970402\n",
      "for {'C': 2, 'solver': 'newton-cg'} => mean : 0.7472623927823733\n",
      "for {'C': 2, 'solver': 'sag'} => mean : 0.9512054120970402\n",
      "for {'C': 3, 'solver': 'newton-cg'} => mean : 0.7436200373925544\n",
      "for {'C': 3, 'solver': 'sag'} => mean : 0.9512054120970402\n",
      "LogisticRegression(C=1, n_jobs=-1, solver='sag')\n"
     ]
    }
   ],
   "source": [
    "params = dict(solver=['newton-cg', 'sag'], C=[1, 2, 3])\n",
    "lr = LogisticRegression(n_jobs=-1)\n",
    "grid = GridSearchCV(lr, params, cv=3, n_jobs=-1)\n",
    "grid.fit(train_x, train_y)\n",
    "grid.cv_results_\n",
    "print(lr)\n",
    "print(\"\\n\".join([f\"for {x} => mean : {y}\" for x,y in zip(\n",
    "                     grid.cv_results_['params'], grid.cv_results_['mean_test_score']\n",
    "                 )]))\n",
    "print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "667a74e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('saved_model/ulr.sav', 'wb') as f:\n",
    "    pickle.dump(grid.best_estimator_, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486329e8",
   "metadata": {},
   "source": [
    "it appears that there is a tiny increasing with this dataset of underlying variable, let's test it on \n",
    "our best model, the MLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "85cb93b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9693010348615367"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = MLPClassifier()\n",
    "mlp.fit(train_x, train_y)\n",
    "mlp.score(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d8255e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('saved_model/umlp.sav', 'wb') as f:\n",
    "    pickle.dump(mlp, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162e30a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
